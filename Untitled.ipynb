{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "638098d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Using cached streamlit-1.22.0-py2.py3-none-any.whl (8.9 MB)\n",
      "Collecting validators>=0.2\n",
      "  Using cached validators-0.20.0.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: python-dateutil in d:\\aman-ds\\lib\\site-packages (from streamlit) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in d:\\aman-ds\\lib\\site-packages (from streamlit) (4.4.0)\n",
      "Collecting gitpython!=3.1.19\n",
      "  Using cached GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
      "Requirement already satisfied: requests>=2.4 in d:\\aman-ds\\lib\\site-packages (from streamlit) (2.28.1)\n",
      "Requirement already satisfied: numpy in d:\\aman-ds\\lib\\site-packages (from streamlit) (1.23.5)\n",
      "Collecting cachetools>=4.0\n",
      "  Using cached cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pydeck>=0.1.dev5\n",
      "  Using cached pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
      "Collecting pyarrow>=4.0\n",
      "  Downloading pyarrow-12.0.0-cp310-cp310-win_amd64.whl (21.5 MB)\n",
      "     ---------------------------------------- 21.5/21.5 MB 2.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: toml in d:\\aman-ds\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: watchdog in d:\\aman-ds\\lib\\site-packages (from streamlit) (2.1.6)\n",
      "Requirement already satisfied: click>=7.0 in d:\\aman-ds\\lib\\site-packages (from streamlit) (8.0.4)\n",
      "Collecting rich>=10.11.0\n",
      "  Using cached rich-13.3.5-py3-none-any.whl (238 kB)\n",
      "Requirement already satisfied: tenacity<9,>=8.0.0 in d:\\aman-ds\\lib\\site-packages (from streamlit) (8.0.1)\n",
      "Collecting pympler>=0.9\n",
      "  Using cached Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
      "Collecting blinker>=1.0.0\n",
      "  Using cached blinker-1.6.2-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in d:\\aman-ds\\lib\\site-packages (from streamlit) (4.11.3)\n",
      "Collecting altair<5,>=3.2.0\n",
      "  Using cached altair-4.2.2-py3-none-any.whl (813 kB)\n",
      "Requirement already satisfied: pandas<3,>=0.25 in d:\\aman-ds\\lib\\site-packages (from streamlit) (1.5.3)\n",
      "Requirement already satisfied: packaging>=14.1 in d:\\aman-ds\\lib\\site-packages (from streamlit) (22.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\aman-ds\\lib\\site-packages (from streamlit) (9.4.0)\n",
      "Requirement already satisfied: tornado>=6.0.3 in d:\\aman-ds\\lib\\site-packages (from streamlit) (6.1)\n",
      "Collecting tzlocal>=1.1\n",
      "  Using cached tzlocal-4.3-py3-none-any.whl (20 kB)\n",
      "Collecting protobuf<4,>=3.12\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-win_amd64.whl (904 kB)\n",
      "     -------------------------------------- 904.0/904.0 kB 2.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jsonschema>=3.0 in d:\\aman-ds\\lib\\site-packages (from altair<5,>=3.2.0->streamlit) (4.17.3)\n",
      "Requirement already satisfied: entrypoints in d:\\aman-ds\\lib\\site-packages (from altair<5,>=3.2.0->streamlit) (0.4)\n",
      "Requirement already satisfied: toolz in d:\\aman-ds\\lib\\site-packages (from altair<5,>=3.2.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: jinja2 in d:\\aman-ds\\lib\\site-packages (from altair<5,>=3.2.0->streamlit) (3.1.2)\n",
      "Requirement already satisfied: colorama in d:\\aman-ds\\lib\\site-packages (from click>=7.0->streamlit) (0.4.6)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Using cached gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\aman-ds\\lib\\site-packages (from importlib-metadata>=1.4->streamlit) (3.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\aman-ds\\lib\\site-packages (from pandas<3,>=0.25->streamlit) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in d:\\aman-ds\\lib\\site-packages (from python-dateutil->streamlit) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\aman-ds\\lib\\site-packages (from requests>=2.4->streamlit) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\aman-ds\\lib\\site-packages (from requests>=2.4->streamlit) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\aman-ds\\lib\\site-packages (from requests>=2.4->streamlit) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\aman-ds\\lib\\site-packages (from requests>=2.4->streamlit) (1.26.14)\n",
      "Collecting markdown-it-py<3.0.0,>=2.2.0\n",
      "  Using cached markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0\n",
      "  Using cached Pygments-2.15.1-py3-none-any.whl (1.1 MB)\n",
      "Collecting pytz-deprecation-shim\n",
      "  Using cached pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tzdata\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: decorator>=3.4.0 in d:\\aman-ds\\lib\\site-packages (from validators>=0.2->streamlit) (5.1.1)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\aman-ds\\lib\\site-packages (from jinja2->altair<5,>=3.2.0->streamlit) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in d:\\aman-ds\\lib\\site-packages (from jsonschema>=3.0->altair<5,>=3.2.0->streamlit) (22.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in d:\\aman-ds\\lib\\site-packages (from jsonschema>=3.0->altair<5,>=3.2.0->streamlit) (0.18.0)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: validators\n",
      "  Building wheel for validators (setup.py): started\n",
      "  Building wheel for validators (setup.py): finished with status 'done'\n",
      "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19591 sha256=e7fa3ef3476921967c7b95d59c26f8b9528b4ac0c42cb2477336ba7f288d57b4\n",
      "  Stored in directory: c:\\users\\aniket\\appdata\\local\\pip\\cache\\wheels\\2d\\55\\25\\123071088f4e466746cbadc923b1a31e08cea99ea9ef6bb35e\n",
      "Successfully built validators\n",
      "Installing collected packages: validators, tzdata, smmap, pympler, pygments, pyarrow, protobuf, mdurl, cachetools, blinker, pytz-deprecation-shim, pydeck, markdown-it-py, gitdb, tzlocal, rich, gitpython, altair, streamlit\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.11.2\n",
      "    Uninstalling Pygments-2.11.2:\n",
      "      Successfully uninstalled Pygments-2.11.2\n",
      "Successfully installed altair-4.2.2 blinker-1.6.2 cachetools-5.3.0 gitdb-4.0.10 gitpython-3.1.31 markdown-it-py-2.2.0 mdurl-0.1.2 protobuf-3.20.3 pyarrow-12.0.0 pydeck-0.8.1b0 pygments-2.15.1 pympler-1.0.1 pytz-deprecation-shim-0.1.0.post0 rich-13.3.5 smmap-5.0.0 streamlit-1.22.0 tzdata-2023.3 tzlocal-4.3 validators-0.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39c6f71f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'vectorizer.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 35\u001b[0m\n\u001b[0;32m     31\u001b[0m         y\u001b[38;5;241m.\u001b[39mappend(ps\u001b[38;5;241m.\u001b[39mstem(i))\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(y)\n\u001b[1;32m---> 35\u001b[0m tfidf \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvectorizer.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     36\u001b[0m model \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     38\u001b[0m st\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmail/SMS Spam Classifier\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Aman-DS\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vectorizer.pkl'"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pickle\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "def transform_text(text):\n",
    "    text = text.lower()\n",
    "    text = nltk.word_tokenize(text)\n",
    "\n",
    "    y = []\n",
    "    for i in text:\n",
    "        if i.isalnum():\n",
    "            y.append(i)\n",
    "\n",
    "    text = y[:]\n",
    "    y.clear()\n",
    "\n",
    "    for i in text:\n",
    "        if i not in stopwords.words('english') and i not in string.punctuation:\n",
    "            y.append(i)\n",
    "\n",
    "    text = y[:]\n",
    "    y.clear()\n",
    "\n",
    "    for i in text:\n",
    "        y.append(ps.stem(i))\n",
    "\n",
    "    return \" \".join(y)\n",
    "\n",
    "tfidf = pickle.load(open('vectorizer.pkl','rb'))\n",
    "model = pickle.load(open('model.pkl','rb'))\n",
    "\n",
    "st.title(\"Email/SMS Spam Classifier\")\n",
    "\n",
    "input_sms = st.text_area(\"Enter the message\")\n",
    "\n",
    "if st.button('Predict'):\n",
    "\n",
    "    # 1. preprocess\n",
    "    transformed_sms = transform_text(input_sms)\n",
    "    # 2. vectorize\n",
    "    vector_input = tfidf.transform([transformed_sms])\n",
    "    # 3. predict\n",
    "    result = model.predict(vector_input)[0]\n",
    "    # 4. Display\n",
    "    if result == 1:\n",
    "        st.header(\"Spam\")\n",
    "    else:\n",
    "        st.header(\"Not Spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0a529b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
